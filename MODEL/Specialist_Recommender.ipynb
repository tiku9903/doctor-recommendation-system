{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c3ee6a-a084-40bf-889d-b1bbb62dfb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 00:33:07.217086: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-29 00:33:08.820410: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /home/tanmay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import pipeline\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import textacy\n",
    "from textacy import preprocessing\n",
    "import spacy\n",
    "import re\n",
    "import textacy.preprocessing as tp\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "model_dump_dir = \"./models\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_dump_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_dump_dir)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c3b1949-ab9c-43ad-bca2-7cc9f3c169b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"Narrativaai/BioGPT-Large-finetuned-chatdoctor\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT-Large\")\n",
    "# device = torch.device('cuda:0')\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # device = torch.device(\"cuda\")\n",
    "\n",
    "# def answer_question(\n",
    "#         prompt,\n",
    "#         temperature=0.1,\n",
    "#         top_p=0.75,\n",
    "#         top_k=40,\n",
    "#         num_beams=2,\n",
    "#         **kwargs,\n",
    "# ):\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "#     input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
    "#     attention_mask = inputs[\"attention_mask\"].to(\"cuda\")\n",
    "#     generation_config = GenerationConfig(\n",
    "#         temperature=temperature,\n",
    "#         top_p=top_p,\n",
    "#         top_k=top_k,\n",
    "#         num_beams=num_beams,\n",
    "#         **kwargs,\n",
    "#     )\n",
    "#     with torch.no_grad():\n",
    "#         generation_output = model.generate(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             generation_config=generation_config,\n",
    "#             return_dict_in_generate=True,\n",
    "#             output_scores=True,\n",
    "#             max_new_tokens=512,\n",
    "#             eos_token_id=tokenizer.eos_token_id\n",
    "\n",
    "#         )\n",
    "#     s = generation_output.sequences[0]\n",
    "#     output = tokenizer.decode(s, skip_special_tokens=True)\n",
    "#     return output.split(\" Response:\")[1]\n",
    "\n",
    "# example_prompt = \"\"\"\n",
    "# Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.\n",
    "\n",
    "# ### Instruction:\n",
    "# If you are a doctor, please answer the medical questions based on the patient's description.\n",
    "\n",
    "# ### Input:\n",
    "# My friend is a gay and had sex with another man. When he woke up this morning he complained about immense pain in his anus and some burning sensation also. Later he found that the person he had sex with is HIV positive. Please advise.\n",
    "\n",
    "# ### Response:\n",
    "# \"\"\"\n",
    "\n",
    "# print(answer_question(example_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba4a6516-82f6-4633-ad80-ece9825c0d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "[WP] I have cancer. Whom should I consult and what medications should I take? \n",
      "[RESPONSE]Hi, Thanks for writing in. Cancer of the salivary gland is a difficult condition to treat, and you should consult a cancer surgeon immediately if you develop any other symptoms due to this condition. There are many chemotherapy Chat Doctor.  They are given in many cases and can be taken safely. If the cancer is in the initial stage then it is a stage where it is operable and cured completely in a few days. However, you should get your CA 125 done and then take treatment under the guidance of an oncologist. There is no need to take chemotherapy as a regular treatment, and you have to discuss with your oncologist on the above treatments. Please do not worry.\n",
      "\n",
      "\n",
      "1: \n",
      "[WP] I have cancer. Whom should I consult and what medications should I take? \n",
      "[RESPONSE]Hello, Thanks for writing to us. If you had cervical lymph node cancer it would have helped us in recommending treatment. The lymph node tuberculosis is confirmed clinically. However, you may try Tab Fexofenadine or Tab Biotin for 5 to 6 weeks. You may consult your oncologist for the same. Hope this helps you. Take care\n",
      "\n",
      "\n",
      "2: \n",
      "[WP] I have cancer. Whom should I consult and what medications should I take? \n",
      "[RESPONSE]I have gone through the details provided by you and I can understand your concerns. If you had been treated for stage 4 cancer in your area then it is likely stage 4 cancer. Only thing is that it has not spread to other areas of the body. So you need not be worried at all. Cancer in any cancer patient is advanced disease and has a poor prognosis. All you need is chemotherapy to prevent cure and prolong survival. In cases where the tumor grows in stage I don't have enough options and chemotherapy is a must. Hope I have answered your query. You can contact me for treatment options. Let me know if I can assist you further. Regards Chat Doctor.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test on sample\n",
    "model.eval()\n",
    "input_prompt = \"I have cancer. Whom should I consult and what medications should I take?\"\n",
    "prompt = f\"\\n<|startoftext|>[WP] {input_prompt} \\n[RESPONSE]\"\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated, \n",
    "                                #bos_token_id=random.randint(1,30000),\n",
    "                                do_sample=True,   \n",
    "                                top_k=50, \n",
    "                                max_length = 800,\n",
    "                                top_p=0.95, \n",
    "                                num_return_sequences=3\n",
    "                                )\n",
    "response_string=\"\"\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "    response_string+=str(tokenizer.decode(sample_output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f10ff1fd-1065-400c-9f1f-caf5b6835468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n[WP] I have cancer. Whom should I consult and what medications should I take? \\n[RESPONSE]Hi, Thanks for writing in. Cancer of the salivary gland is a difficult condition to treat, and you should consult a cancer surgeon immediately if you develop any other symptoms due to this condition. There are many chemotherapy Chat Doctor.  They are given in many cases and can be taken safely. If the cancer is in the initial stage then it is a stage where it is operable and cured completely in a few days. However, you should get your CA 125 done and then take treatment under the guidance of an oncologist. There is no need to take chemotherapy as a regular treatment, and you have to discuss with your oncologist on the above treatments. Please do not worry.\\n[WP] I have cancer. Whom should I consult and what medications should I take? \\n[RESPONSE]Hello, Thanks for writing to us. If you had cervical lymph node cancer it would have helped us in recommending treatment. The lymph node tuberculosis is confirmed clinically. However, you may try Tab Fexofenadine or Tab Biotin for 5 to 6 weeks. You may consult your oncologist for the same. Hope this helps you. Take care\\n[WP] I have cancer. Whom should I consult and what medications should I take? \\n[RESPONSE]I have gone through the details provided by you and I can understand your concerns. If you had been treated for stage 4 cancer in your area then it is likely stage 4 cancer. Only thing is that it has not spread to other areas of the body. So you need not be worried at all. Cancer in any cancer patient is advanced disease and has a poor prognosis. All you need is chemotherapy to prevent cure and prolong survival. In cases where the tumor grows in stage I don't have enough options and chemotherapy is a must. Hope I have answered your query. You can contact me for treatment options. Let me know if I can assist you further. Regards Chat Doctor.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "971f9bdc-6b8c-4138-ae5d-597b455fad9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition days tumor cases symptoms stage details enough options area lymph completely treatments immediately guidance clinically RESPONSE salivary tuberculosis body many other survival regular weeks patient advanced likely cervical node safely Only treatment However cure surgeon medications then difficult cancer query above oncologist disease need initial concerns Thanks thing same all So prolong chemotherapy care areas operable further poor prognosis gland few WP Hope\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=(\"parser\",))\n",
    "def extract_nouns_verbs(text):\n",
    "    text = tp.normalize.whitespace(text)\n",
    "    text = tp.remove.punctuation(text)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "\n",
    "    unique_nouns_verbs = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\",\"ADJ\",\"ADV\"]:\n",
    "            unique_nouns_verbs.add(token.text)\n",
    "    \n",
    "    return \" \".join(unique_nouns_verbs)\n",
    "response_keywords=str(extract_nouns_verbs(response_string))\n",
    "print(response_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73451a01-6d8a-482e-af7b-b47cf3b95f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"disease_description.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c327037-924b-4bd5-8c43-5ef387255b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>disease</th>\n",
       "      <th>Symptom</th>\n",
       "      <th>reason</th>\n",
       "      <th>TestsAndProcedures</th>\n",
       "      <th>commonMedications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Panic disorder</td>\n",
       "      <td>['Anxiety and nervousness', 'Depression', 'Sho...</td>\n",
       "      <td>Panic disorder is an anxiety disorder characte...</td>\n",
       "      <td>['Psychotherapy', 'Mental health counseling', ...</td>\n",
       "      <td>['Lorazepam', 'Alprazolam (Xanax)', 'Clonazepa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Vocal cord polyp</td>\n",
       "      <td>['Hoarse voice', 'Sore throat', 'Difficulty sp...</td>\n",
       "      <td>beclomethasone nasal product,</td>\n",
       "      <td>['Tracheoscopy and laryngoscopy with biopsy', ...</td>\n",
       "      <td>['Esomeprazole (Nexium)', 'Beclomethasone Nasa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Turner syndrome</td>\n",
       "      <td>['Groin mass', 'Leg pain', 'Hip pain', 'Suprap...</td>\n",
       "      <td>Turner syndrome or Ullrich\\xe2\\x80\\x93Turner s...</td>\n",
       "      <td>['Complete physical skin exam performed (ML)',...</td>\n",
       "      <td>['Somatropin', 'Sulfamethoxazole (Bactrim)', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Cryptorchidism</td>\n",
       "      <td>['Symptoms of the scrotum and testes', 'Swelli...</td>\n",
       "      <td>Cryptorchidism (derived from the Greek \\xce\\xb...</td>\n",
       "      <td>['Complete physical skin exam performed (ML)',...</td>\n",
       "      <td>['Haemophilus B Conjugate Vaccine (Obsolete)',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Poisoning due to ethylene glycol</td>\n",
       "      <td>['Abusing alcohol', 'Fainting', 'Hostile behav...</td>\n",
       "      <td>thiamine,</td>\n",
       "      <td>['Intravenous fluid replacement', 'Hematologic...</td>\n",
       "      <td>['Lorazepam', 'Thiamine', 'Naloxone (Suboxone)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                           disease  \\\n",
       "0    0                    Panic disorder   \n",
       "1    1                  Vocal cord polyp   \n",
       "2    2                   Turner syndrome   \n",
       "3    3                    Cryptorchidism   \n",
       "4    4  Poisoning due to ethylene glycol   \n",
       "\n",
       "                                             Symptom  \\\n",
       "0  ['Anxiety and nervousness', 'Depression', 'Sho...   \n",
       "1  ['Hoarse voice', 'Sore throat', 'Difficulty sp...   \n",
       "2  ['Groin mass', 'Leg pain', 'Hip pain', 'Suprap...   \n",
       "3  ['Symptoms of the scrotum and testes', 'Swelli...   \n",
       "4  ['Abusing alcohol', 'Fainting', 'Hostile behav...   \n",
       "\n",
       "                                              reason  \\\n",
       "0  Panic disorder is an anxiety disorder characte...   \n",
       "1                     beclomethasone nasal product,    \n",
       "2  Turner syndrome or Ullrich\\xe2\\x80\\x93Turner s...   \n",
       "3  Cryptorchidism (derived from the Greek \\xce\\xb...   \n",
       "4                                         thiamine,    \n",
       "\n",
       "                                  TestsAndProcedures  \\\n",
       "0  ['Psychotherapy', 'Mental health counseling', ...   \n",
       "1  ['Tracheoscopy and laryngoscopy with biopsy', ...   \n",
       "2  ['Complete physical skin exam performed (ML)',...   \n",
       "3  ['Complete physical skin exam performed (ML)',...   \n",
       "4  ['Intravenous fluid replacement', 'Hematologic...   \n",
       "\n",
       "                                   commonMedications  \n",
       "0  ['Lorazepam', 'Alprazolam (Xanax)', 'Clonazepa...  \n",
       "1  ['Esomeprazole (Nexium)', 'Beclomethasone Nasa...  \n",
       "2  ['Somatropin', 'Sulfamethoxazole (Bactrim)', '...  \n",
       "3  ['Haemophilus B Conjugate Vaccine (Obsolete)',...  \n",
       "4  ['Lorazepam', 'Thiamine', 'Naloxone (Suboxone)...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27911536-043f-4b8a-8c45-9df12b5612c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idx                    int64\n",
       "disease               object\n",
       "Symptom               object\n",
       "reason                object\n",
       "TestsAndProcedures    object\n",
       "commonMedications     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78301a-5b2c-4281-a9c8-731148739306",
   "metadata": {},
   "source": [
    "## Handling score for disease speciality csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee4dcc2-ae79-47b5-b2a6-4876a4cab94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def handle_diseases_data(document_text):\n",
    "#     document_text = preprocessing.normalize.whitespace(document_text)\n",
    "#     document_text = preprocessing.remove.punctuation(document_text)\n",
    "    \n",
    "#     en = spacy.load(\"en_core_web_sm\", disable=(\"parser\",))\n",
    "#     doc = en(document_text)\n",
    "\n",
    "#     nouns_verbs = [(token.text, token.pos_) for token in doc if token.pos_ in [\"NOUN\", \"ADJ\"]]\n",
    "#     return nouns_verbs\n",
    "\n",
    "# # Initialize a list to store DataFrames of appended keywords\n",
    "# appended_keywords_dfs = []\n",
    "\n",
    "# # Get the maximum number of keywords to fill missing entries\n",
    "# max_keywords = 0\n",
    "\n",
    "# # Iterate through each row of the DataFrame\n",
    "# for index, row in df.iterrows():\n",
    "#     # Apply the handle_diseases_data function to extract keywords from each column\n",
    "#     keywords = {}\n",
    "#     for column in df.columns:\n",
    "#         if column != 'idx':  # Exclude the 'idx' column\n",
    "#             extracted_keywords = []\n",
    "#             if isinstance(row[column], list):\n",
    "#                 for item in row[column]:\n",
    "#                     extracted_keywords.extend(handle_diseases_data(item))\n",
    "#             else:\n",
    "#                 extracted_keywords.extend(handle_diseases_data(str(row[column])))\n",
    "            \n",
    "#             keywords[column] = extracted_keywords\n",
    "            \n",
    "#             # Update max_keywords if necessary\n",
    "#             max_keywords = max(max_keywords, len(extracted_keywords))\n",
    "    \n",
    "#     # Convert keywords dictionary to a DataFrame and append it to the list\n",
    "#     appended_keywords_dfs.append(pd.DataFrame(keywords))\n",
    "\n",
    "# # Fill missing entries with a placeholder\n",
    "# for df_keywords in appended_keywords_dfs:\n",
    "#     for column in df_keywords.columns:\n",
    "#         current_length = len(df_keywords[column])\n",
    "#         if current_length < max_keywords:\n",
    "#             df_keywords[column] += [('', '')] * (max_keywords - current_length)\n",
    "\n",
    "# # Concatenate all DataFrames in the list along axis 1 (columns)\n",
    "# appended_keywords_df = pd.concat(appended_keywords_dfs, axis=1)\n",
    "\n",
    "# # Save the new DataFrame to a CSV file\n",
    "# appended_keywords_df.to_csv('appending_keywords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40039064-dbd1-459b-a3f9-a80a2451451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_metrics = {\n",
    "    'disease': 4,\n",
    "    'Symptom': 2,\n",
    "    'reason': 1,\n",
    "    'TestsAndProcedures': 1,\n",
    "    'commonMedications': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89606137-d057-4754-a9ba-a9f26804ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relevance_score(keyword, column_value, column_name):\n",
    "    if isinstance(column_value, list):\n",
    "        for item in column_value:\n",
    "            if keyword in item:\n",
    "                return score_metrics[column_name]  # Return score metric for the column\n",
    "    else:\n",
    "        if keyword in str(column_value):\n",
    "            return score_metrics[column_name]  # Return score metric for the column\n",
    "    return 0  # Return 0 if keyword not found in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f2ff09a-1b79-45a2-b6a2-8bf94984bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = []\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Initialize total score for the current row\n",
    "    total_score = 0\n",
    "    \n",
    "    # Calculate relevance score for each keyword in each column\n",
    "    for keyword in response_keywords:\n",
    "        for column_name in df.columns:\n",
    "            if column_name != 'idx':  # Exclude the 'idx' column\n",
    "                column_value = row[column_name]\n",
    "                score = calculate_relevance_score(keyword, column_value, column_name)\n",
    "                total_score += score\n",
    "    \n",
    "    # Append total score for the current row\n",
    "    total_scores.append(total_score)\n",
    "\n",
    "# Append total scores to the DataFrame\n",
    "df['Total_Score'] = total_scores\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df.to_csv('updated_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94b19e2b-9583-4d6d-a6c9-3e8a2d8a0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df=pd.read_csv(\"updated_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "519a7e79-de45-41cf-8e0d-9eec4ec093ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>disease</th>\n",
       "      <th>Symptom</th>\n",
       "      <th>reason</th>\n",
       "      <th>TestsAndProcedures</th>\n",
       "      <th>commonMedications</th>\n",
       "      <th>Total_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Panic disorder</td>\n",
       "      <td>['Anxiety and nervousness', 'Depression', 'Sho...</td>\n",
       "      <td>Panic disorder is an anxiety disorder characte...</td>\n",
       "      <td>['Psychotherapy', 'Mental health counseling', ...</td>\n",
       "      <td>['Lorazepam', 'Alprazolam (Xanax)', 'Clonazepa...</td>\n",
       "      <td>3506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Vocal cord polyp</td>\n",
       "      <td>['Hoarse voice', 'Sore throat', 'Difficulty sp...</td>\n",
       "      <td>beclomethasone nasal product,</td>\n",
       "      <td>['Tracheoscopy and laryngoscopy with biopsy', ...</td>\n",
       "      <td>['Esomeprazole (Nexium)', 'Beclomethasone Nasa...</td>\n",
       "      <td>3171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Turner syndrome</td>\n",
       "      <td>['Groin mass', 'Leg pain', 'Hip pain', 'Suprap...</td>\n",
       "      <td>Turner syndrome or Ullrich\\xe2\\x80\\x93Turner s...</td>\n",
       "      <td>['Complete physical skin exam performed (ML)',...</td>\n",
       "      <td>['Somatropin', 'Sulfamethoxazole (Bactrim)', '...</td>\n",
       "      <td>3387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Cryptorchidism</td>\n",
       "      <td>['Symptoms of the scrotum and testes', 'Swelli...</td>\n",
       "      <td>Cryptorchidism (derived from the Greek \\xce\\xb...</td>\n",
       "      <td>['Complete physical skin exam performed (ML)',...</td>\n",
       "      <td>['Haemophilus B Conjugate Vaccine (Obsolete)',...</td>\n",
       "      <td>3119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Poisoning due to ethylene glycol</td>\n",
       "      <td>['Abusing alcohol', 'Fainting', 'Hostile behav...</td>\n",
       "      <td>thiamine,</td>\n",
       "      <td>['Intravenous fluid replacement', 'Hematologic...</td>\n",
       "      <td>['Lorazepam', 'Thiamine', 'Naloxone (Suboxone)...</td>\n",
       "      <td>3477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                           disease  \\\n",
       "0    0                    Panic disorder   \n",
       "1    1                  Vocal cord polyp   \n",
       "2    2                   Turner syndrome   \n",
       "3    3                    Cryptorchidism   \n",
       "4    4  Poisoning due to ethylene glycol   \n",
       "\n",
       "                                             Symptom  \\\n",
       "0  ['Anxiety and nervousness', 'Depression', 'Sho...   \n",
       "1  ['Hoarse voice', 'Sore throat', 'Difficulty sp...   \n",
       "2  ['Groin mass', 'Leg pain', 'Hip pain', 'Suprap...   \n",
       "3  ['Symptoms of the scrotum and testes', 'Swelli...   \n",
       "4  ['Abusing alcohol', 'Fainting', 'Hostile behav...   \n",
       "\n",
       "                                              reason  \\\n",
       "0  Panic disorder is an anxiety disorder characte...   \n",
       "1                     beclomethasone nasal product,    \n",
       "2  Turner syndrome or Ullrich\\xe2\\x80\\x93Turner s...   \n",
       "3  Cryptorchidism (derived from the Greek \\xce\\xb...   \n",
       "4                                         thiamine,    \n",
       "\n",
       "                                  TestsAndProcedures  \\\n",
       "0  ['Psychotherapy', 'Mental health counseling', ...   \n",
       "1  ['Tracheoscopy and laryngoscopy with biopsy', ...   \n",
       "2  ['Complete physical skin exam performed (ML)',...   \n",
       "3  ['Complete physical skin exam performed (ML)',...   \n",
       "4  ['Intravenous fluid replacement', 'Hematologic...   \n",
       "\n",
       "                                   commonMedications  Total_Score  \n",
       "0  ['Lorazepam', 'Alprazolam (Xanax)', 'Clonazepa...         3506  \n",
       "1  ['Esomeprazole (Nexium)', 'Beclomethasone Nasa...         3171  \n",
       "2  ['Somatropin', 'Sulfamethoxazole (Bactrim)', '...         3387  \n",
       "3  ['Haemophilus B Conjugate Vaccine (Obsolete)',...         3119  \n",
       "4  ['Lorazepam', 'Thiamine', 'Naloxone (Suboxone)...         3477  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6934a5ad-ff12-45c5-89bb-09412b17526c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2318"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df[\"Total_Score\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d6828-ac89-4d8f-b436-beeb9174db8d",
   "metadata": {},
   "source": [
    "## Doctor To Speciality Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "003788e9-583a-4d59-90cc-9a6c736b0ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Speciality</th>\n",
       "      <th>Description</th>\n",
       "      <th>Subspeciality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Allergy and Immunology</td>\n",
       "      <td>Allergy and immunology is a subspecialty of in...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Anesthesiology</td>\n",
       "      <td>A doctor of anesthesiologyan anesthesiologistp...</td>\n",
       "      <td>['adult cardiothoracic anesthesiology', 'criti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>Cardiology is a subspecialty of internal medic...</td>\n",
       "      <td>['advanced heart failure and transplant cardio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Colon and Rectal Surgery</td>\n",
       "      <td>Colon and rectal surgeons diagnose and treat d...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Dermatology</td>\n",
       "      <td>A doctor of dermatologya dermatologistfinds, p...</td>\n",
       "      <td>['dermatopathology', 'micrographic surgery', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                Speciality  \\\n",
       "0   1    Allergy and Immunology   \n",
       "1   2            Anesthesiology   \n",
       "2   3                Cardiology   \n",
       "3   4  Colon and Rectal Surgery   \n",
       "4   5               Dermatology   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Allergy and immunology is a subspecialty of in...   \n",
       "1  A doctor of anesthesiologyan anesthesiologistp...   \n",
       "2  Cardiology is a subspecialty of internal medic...   \n",
       "3  Colon and rectal surgeons diagnose and treat d...   \n",
       "4  A doctor of dermatologya dermatologistfinds, p...   \n",
       "\n",
       "                                       Subspeciality  \n",
       "0                                                NaN  \n",
       "1  ['adult cardiothoracic anesthesiology', 'criti...  \n",
       "2  ['advanced heart failure and transplant cardio...  \n",
       "3                                                NaN  \n",
       "4  ['dermatopathology', 'micrographic surgery', '...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specialist_df = pd.read_csv(\"specialist_description.csv\")\n",
    "specialist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98318e63-b6cc-4fa0-97a1-a4199d3b723a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 specialists:\n",
      "1. Oncology: 341055 score\n",
      "2. Obstetrics and Gynecology: 230 score\n",
      "3. Allergy and Immunology: 220 score\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store scores for each specialist\n",
    "scores = {}\n",
    "\n",
    "# Split the response string into words and convert to lowercase\n",
    "response_words = response_string.lower().split()\n",
    "\n",
    "# Iterate over each word in the response string\n",
    "for word in response_words:\n",
    "    # Check if the word is in the \"Speciality\" column of speciality_df\n",
    "    for index, row in specialist_df.iterrows():\n",
    "        if word == row['Speciality'].lower() and word != \"medicine\" and word!=\"and\":\n",
    "            # Increment score by 1000\n",
    "            scores[row['Speciality']] = scores.get(row['Speciality'], 0) + 1000\n",
    "        elif row['Speciality'].lower().endswith(\"ogy\"):\n",
    "            # Check if the specialty ends with \"ogy\" and the corresponding term is in the response string\n",
    "            specialty_term = row['Speciality'].lower()[:-3] + \"ogist\"\n",
    "            if specialty_term in response_string.lower():\n",
    "                scores[row['Speciality']] = scores.get(row['Speciality'], 0) + 1000\n",
    "\n",
    "# Iterate over each description in speciality_df\n",
    "for index, row in specialist_df.iterrows():\n",
    "    description_words = row['Description'].lower().split()\n",
    "    # Count occurrences of each word in the response string\n",
    "    for word in description_words:\n",
    "        if word in response_words:\n",
    "            # Increase score by 5 for each hit\n",
    "            scores[row['Speciality']] = scores.get(row['Speciality'], 0) + 5\n",
    "\n",
    "# Sort specialists based on scores\n",
    "sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 3 specialists with the highest score\n",
    "print(\"Top 3 specialists:\")\n",
    "for i, (speciality, score) in enumerate(sorted_scores[:3], 1):\n",
    "    print(f\"{i}. {speciality}: {score} score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0caaa687-f114-4c6a-841d-2f17ecca44dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 0                   Allergy and Immunology\n",
       "1                           Anesthesiology\n",
       "2                               Cardiology\n",
       "3                 Colon and Rectal Surgery\n",
       "4                              Dermatology\n",
       "5                       Emergency Medicine\n",
       "6                          Family Medicine\n",
       "7                       Forensic Pathology\n",
       "8                          General Surgery\n",
       "9                    Genetics and Genomics\n",
       "10         Hospice and Palliative Medicine\n",
       "11                       Hospital Medicine\n",
       "12                       Internal Medicine\n",
       "13                               Neurology\n",
       "14                    Neurological Surgery\n",
       "15               Obstetrics and Gynecology\n",
       "16                      Ophthalmic Surgery\n",
       "17                     Orthopaedic Surgery\n",
       "18                          Otolaryngology\n",
       "19                               Pathology\n",
       "20                              Pediatrics\n",
       "21    Physical Medicine and Rehabilitation\n",
       "22                         Plastic Surgery\n",
       "23                     Preventive Medicine\n",
       "24                              Psychiatry\n",
       "25                               Radiology\n",
       "26                            Rheumatology\n",
       "27                          Sleep Medicine\n",
       "28                        Thoracic Surgery\n",
       "29                                 Urology\n",
       "30                        Vascular Surgery\n",
       "31                                Oncology\n",
       "32                           Endocrinology\n",
       "33                              Nephrology\n",
       "34                             Pulmonology\n",
       "35                        Gastroenterology\n",
       "Name: Speciality, dtype: object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specialist_df[\"Speciality\"].unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ee3dc-2e72-434a-b746-6b699db9c7be",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df9e0164-e86e-4441-a409-1e517b2a7072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba3ce1f7-4fab-425b-9f12-c83cf7df77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cbb7358-bc56-4135-9ad6-395749f41c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              disease most_similar_speciality\n",
      "0                      Panic disorder              Pediatrics\n",
      "1                    Vocal cord polyp  Allergy and Immunology\n",
      "2                     Turner syndrome   Genetics and Genomics\n",
      "3                      Cryptorchidism             Dermatology\n",
      "4    Poisoning due to ethylene glycol  Allergy and Immunology\n",
      "..                                ...                     ...\n",
      "791                    Kaposi sarcoma             Dermatology\n",
      "792                 Spondylolisthesis    Neurological Surgery\n",
      "793               Pseudotumor cerebri      Ophthalmic Surgery\n",
      "794       Conjunctivitis due to virus      Ophthalmic Surgery\n",
      "795            Open wound of the nose  Allergy and Immunology\n",
      "\n",
      "[796 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to disease descriptions\n",
    "df['clean_description'] = df['reason'].apply(preprocess_text)\n",
    "# Apply preprocessing to specialist descriptions\n",
    "specialist_df['clean_description'] = specialist_df['Description'].apply(preprocess_text)\n",
    "\n",
    "# Create TF-IDF vectors for disease and specialist descriptions\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "disease_tfidf = tfidf_vectorizer.fit_transform(df['clean_description'])\n",
    "specialist_tfidf = tfidf_vectorizer.transform(specialist_df['clean_description'])\n",
    "\n",
    "# Calculate cosine similarity between disease and specialist descriptions\n",
    "similarity_matrix = cosine_similarity(disease_tfidf, specialist_tfidf)\n",
    "\n",
    "# Get the index of the specialty with the highest similarity score for each disease\n",
    "max_similarity_index = similarity_matrix.argmax(axis=1)\n",
    "\n",
    "# Report the specialty name with the highest similarity score for each disease\n",
    "df['most_similar_speciality'] = specialist_df.iloc[max_similarity_index]['Speciality'].values\n",
    "\n",
    "# Display the result\n",
    "print(df[['disease', 'most_similar_speciality']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efcb5c-449b-4322-a678-2ce5c606d6c4",
   "metadata": {},
   "source": [
    "### KL-D method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03510a9b-6d0d-47ec-bb99-0aa08dfe1ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              disease    most_similar_speciality\n",
      "0                      Panic disorder  Obstetrics and Gynecology\n",
      "1                    Vocal cord polyp     Allergy and Immunology\n",
      "2                     Turner syndrome            Family Medicine\n",
      "3                      Cryptorchidism                   Oncology\n",
      "4    Poisoning due to ethylene glycol     Allergy and Immunology\n",
      "..                                ...                        ...\n",
      "791                    Kaposi sarcoma             Anesthesiology\n",
      "792                 Spondylolisthesis                 Cardiology\n",
      "793               Pseudotumor cerebri                  Neurology\n",
      "794       Conjunctivitis due to virus             Anesthesiology\n",
      "795            Open wound of the nose     Allergy and Immunology\n",
      "\n",
      "[796 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Function to calculate KL Divergence\n",
    "def kl_divergence(p, q):\n",
    "    return entropy(p, q)\n",
    "\n",
    "# Calculate KL Divergence between disease and specialist descriptions\n",
    "kl_divergence_matrix = np.zeros((len(df), len(specialist_df)))\n",
    "for i, disease_desc in enumerate(df['clean_description']):\n",
    "    for j, specialist_desc in enumerate(specialist_df['clean_description']):\n",
    "        disease_words = set(disease_desc.split())\n",
    "        specialist_words = set(specialist_desc.split())\n",
    "        \n",
    "        # Calculate word frequency distributions\n",
    "        disease_word_freq = {word: disease_desc.count(word) / len(disease_words) for word in disease_words}\n",
    "        specialist_word_freq = {word: specialist_desc.count(word) / len(specialist_words) for word in disease_words}\n",
    "        \n",
    "        # Convert word frequencies to arrays\n",
    "        p = np.array(list(disease_word_freq.values()))\n",
    "        q = np.array([specialist_word_freq.get(word, 0) for word in disease_words])\n",
    "        \n",
    "        # Add epsilon to avoid divide by zero\n",
    "        p += np.finfo(float).eps\n",
    "        q += np.finfo(float).eps\n",
    "        \n",
    "        # Calculate KL Divergence\n",
    "        kl_divergence_matrix[i][j] = kl_divergence(p, q)\n",
    "\n",
    "# Get the index of the specialty with the lowest KL Divergence for each disease\n",
    "min_kl_divergence_index = kl_divergence_matrix.argmin(axis=1)\n",
    "\n",
    "# Report the specialty name with the lowest KL Divergence for each disease\n",
    "df['most_similar_speciality'] = specialist_df.iloc[min_kl_divergence_index]['Speciality'].values\n",
    "\n",
    "# Display the result\n",
    "print(df[['disease', 'most_similar_speciality']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e58e52e-1673-434d-96a7-06b7865caf7a",
   "metadata": {},
   "source": [
    "### Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbf5e5ce-e935-44ac-abbb-eac252220d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanmay/anaconda3/lib/python3.11/site-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              disease most_similar_speciality\n",
      "0                      Panic disorder      Ophthalmic Surgery\n",
      "1                    Vocal cord polyp  Allergy and Immunology\n",
      "2                     Turner syndrome         Family Medicine\n",
      "3                      Cryptorchidism                Oncology\n",
      "4    Poisoning due to ethylene glycol  Allergy and Immunology\n",
      "..                                ...                     ...\n",
      "791                    Kaposi sarcoma          Anesthesiology\n",
      "792                 Spondylolisthesis              Cardiology\n",
      "793               Pseudotumor cerebri              Pediatrics\n",
      "794       Conjunctivitis due to virus          Anesthesiology\n",
      "795            Open wound of the nose  Allergy and Immunology\n",
      "\n",
      "[796 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Calculate Pearson correlation coefficient between disease and specialist descriptions\n",
    "pearson_corr_matrix = np.zeros((len(df), len(specialist_df)))\n",
    "for i, disease_desc in enumerate(df['clean_description']):\n",
    "    for j, specialist_desc in enumerate(specialist_df['clean_description']):\n",
    "        disease_words = set(disease_desc.split())\n",
    "        specialist_words = set(specialist_desc.split())\n",
    "        \n",
    "        # Create word frequency dictionaries\n",
    "        disease_word_freq = {word: disease_desc.count(word) for word in disease_words}\n",
    "        specialist_word_freq = {word: specialist_desc.count(word) for word in disease_words}\n",
    "        \n",
    "        # Convert word frequencies to arrays\n",
    "        p = np.array(list(disease_word_freq.values()))\n",
    "        q = np.array([specialist_word_freq.get(word, 0) for word in disease_words])\n",
    "        \n",
    "        # Pad arrays with zeros to ensure length >= 2\n",
    "        if len(p) < 2:\n",
    "            p = np.pad(p, (0, 2 - len(p)), 'constant')\n",
    "        if len(q) < 2:\n",
    "            q = np.pad(q, (0, 2 - len(q)), 'constant')\n",
    "        \n",
    "        # Calculate Pearson correlation coefficient\n",
    "        pearson_corr_matrix[i][j], _ = pearsonr(p, q)\n",
    "\n",
    "# Get the index of the specialty with the highest Pearson correlation coefficient for each disease\n",
    "max_pearson_corr_index = pearson_corr_matrix.argmax(axis=1)\n",
    "\n",
    "# Report the specialty name with the highest Pearson correlation coefficient for each disease\n",
    "df['most_similar_speciality'] = specialist_df.iloc[max_pearson_corr_index]['Speciality'].values\n",
    "\n",
    "# Display the result\n",
    "print(df[['disease', 'most_similar_speciality']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09f014-75c1-4e32-967d-74c308492121",
   "metadata": {},
   "source": [
    "### Weighted Correlation Co-efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7698ff72-cf58-4d19-988f-650d67c852f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2840969/2908973671.py:14: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  weighted_corrcoef = weighted_covariance / (weighted_std_p * weighted_std_q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              disease most_similar_speciality\n",
      "0                      Panic disorder      Ophthalmic Surgery\n",
      "1                    Vocal cord polyp  Allergy and Immunology\n",
      "2                     Turner syndrome         Family Medicine\n",
      "3                      Cryptorchidism                Oncology\n",
      "4    Poisoning due to ethylene glycol  Allergy and Immunology\n",
      "..                                ...                     ...\n",
      "791                    Kaposi sarcoma          Anesthesiology\n",
      "792                 Spondylolisthesis              Cardiology\n",
      "793               Pseudotumor cerebri              Pediatrics\n",
      "794       Conjunctivitis due to virus          Anesthesiology\n",
      "795            Open wound of the nose  Allergy and Immunology\n",
      "\n",
      "[796 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def weighted_corrcoef(p, q):\n",
    "    # Calculate weighted mean of p and q\n",
    "    weighted_mean_p = np.mean(p)\n",
    "    weighted_mean_q = np.mean(q)\n",
    "    \n",
    "    # Calculate weighted covariance\n",
    "    weighted_covariance = np.sum((p - weighted_mean_p) * (q - weighted_mean_q)) / len(p)\n",
    "    \n",
    "    # Calculate weighted standard deviation of p and q\n",
    "    weighted_std_p = np.std(p)\n",
    "    weighted_std_q = np.std(q)\n",
    "    \n",
    "    # Calculate weighted correlation coefficient\n",
    "    weighted_corrcoef = weighted_covariance / (weighted_std_p * weighted_std_q)\n",
    "    \n",
    "    return weighted_corrcoef\n",
    "\n",
    "# Calculate weighted correlation coefficient between disease and specialist descriptions\n",
    "weighted_corr_matrix = np.zeros((len(df), len(specialist_df)))\n",
    "for i, disease_desc in enumerate(df['clean_description']):\n",
    "    for j, specialist_desc in enumerate(specialist_df['clean_description']):\n",
    "        disease_words = set(disease_desc.split())\n",
    "        specialist_words = set(specialist_desc.split())\n",
    "        \n",
    "        # Create word frequency dictionaries\n",
    "        disease_word_freq = {word: disease_desc.count(word) for word in disease_words}\n",
    "        specialist_word_freq = {word: specialist_desc.count(word) for word in disease_words}\n",
    "        \n",
    "        # Convert word frequencies to arrays\n",
    "        p = np.array(list(disease_word_freq.values()))\n",
    "        q = np.array([specialist_word_freq.get(word, 0) for word in disease_words])\n",
    "        \n",
    "        # Calculate weighted correlation coefficient\n",
    "        weighted_corr_matrix[i][j] = weighted_corrcoef(p, q)\n",
    "\n",
    "# Get the index of the specialty with the highest weighted correlation coefficient for each disease\n",
    "max_weighted_corr_index = weighted_corr_matrix.argmax(axis=1)\n",
    "\n",
    "# Report the specialty name with the highest weighted correlation coefficient for each disease\n",
    "df['most_similar_speciality'] = specialist_df.iloc[max_weighted_corr_index]['Speciality'].values\n",
    "\n",
    "# Display the result\n",
    "print(df[['disease', 'most_similar_speciality']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c94a030-9803-40b4-86b4-3f61a0bfa606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanmay/anaconda3/lib/python3.11/site-packages/datasets/load.py:1461: FutureWarning: The repository for medical_dialog contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/medical_dialog\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c8c1f12ffb47e89b9ace84e8556cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312763c2ffe54a878c1182b27f9587da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ManualDownloadError",
     "evalue": "                    The dataset medical_dialog with config en requires manual data.\n                    Please follow the manual download instructions:\n\n  For English:\nYou need to go to https://drive.google.com/drive/folders/1g29ssimdZ6JzTST6Y8g6h-ogUNReBtJD?usp=sharing,    and manually download the dataset from Google Drive. Once it is completed,\n    a file named Medical-Dialogue-Dataset-English-<timestamp-info>.zip will appear in your Downloads folder(\n    or whichever folder your browser chooses to save files to). Unzip the folder to obtain\n    a folder named \"Medical-Dialogue-Dataset-English\" several text files.\n\n    Now, you can specify the path to this folder for the data_dir argument in the\n    datasets.load_dataset(...) option.\n    The <path/to/folder> can e.g. be \"/Downloads/Medical-Dialogue-Dataset-English\".\n    The data can then be loaded using the below command:         `datasets.load_dataset(\"medical_dialog\", name=\"en\", data_dir=\"/Downloads/Medical-Dialogue-Dataset-English\")`.\n\n\n  For Chinese:\nFollow the above process. Change the 'name' to 'zh'.The download link is https://drive.google.com/drive/folders/1r09_i8nJ9c1nliXVGXwSqRYqklcHd9e2\n\n    **NOTE**\n    - A caution while downloading from drive. It is better to download single files since creating a zip might not include files <500 MB. This has been observed mutiple times.\n    - After downloading the files and adding them to the appropriate folder, the path of the folder can be given as input tu the data_dir path.\n\n                    Manual data can be loaded with:\n                     datasets.load_dataset(\"medical_dialog\", data_dir=\"<path/to/manual/data>\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mManualDownloadError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedical_dialog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/load.py:2582\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2579\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2582\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\n\u001b[1;32m   2583\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   2584\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   2585\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[1;32m   2586\u001b[0m     try_from_hf_gcs\u001b[38;5;241m=\u001b[39mtry_from_hf_gcs,\n\u001b[1;32m   2587\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m   2588\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   2589\u001b[0m )\n\u001b[1;32m   2591\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2592\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2593\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2594\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/builder.py:982\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    979\u001b[0m     _dest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs\u001b[38;5;241m.\u001b[39m_strip_protocol(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir) \u001b[38;5;28;01mif\u001b[39;00m is_local \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir\n\u001b[1;32m    980\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading and preparing dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_manual_download(dl_manager)\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# Create a tmp dir and rename to self._output_dir on successful exit.\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m incomplete_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir) \u001b[38;5;28;01mas\u001b[39;00m tmp_output_dir:\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# Temporarily assign _output_dir to tmp_data_dir to avoid having to forward\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# it to every sub function.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/builder.py:1028\u001b[0m, in \u001b[0;36mDatasetBuilder._check_manual_download\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_manual_download\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mmanual_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1028\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ManualDownloadError(\n\u001b[1;32m   1029\u001b[0m             textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m   1030\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124m                The dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with config \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires manual data.\u001b[39m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;124m                Please follow the manual download instructions:\u001b[39m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;124m                 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124m                Manual data can be loaded with:\u001b[39m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124m                 datasets.load_dataset(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, data_dir=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<path/to/manual/data>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1036\u001b[0m             )\n\u001b[1;32m   1037\u001b[0m         )\n",
      "\u001b[0;31mManualDownloadError\u001b[0m:                     The dataset medical_dialog with config en requires manual data.\n                    Please follow the manual download instructions:\n\n  For English:\nYou need to go to https://drive.google.com/drive/folders/1g29ssimdZ6JzTST6Y8g6h-ogUNReBtJD?usp=sharing,    and manually download the dataset from Google Drive. Once it is completed,\n    a file named Medical-Dialogue-Dataset-English-<timestamp-info>.zip will appear in your Downloads folder(\n    or whichever folder your browser chooses to save files to). Unzip the folder to obtain\n    a folder named \"Medical-Dialogue-Dataset-English\" several text files.\n\n    Now, you can specify the path to this folder for the data_dir argument in the\n    datasets.load_dataset(...) option.\n    The <path/to/folder> can e.g. be \"/Downloads/Medical-Dialogue-Dataset-English\".\n    The data can then be loaded using the below command:         `datasets.load_dataset(\"medical_dialog\", name=\"en\", data_dir=\"/Downloads/Medical-Dialogue-Dataset-English\")`.\n\n\n  For Chinese:\nFollow the above process. Change the 'name' to 'zh'.The download link is https://drive.google.com/drive/folders/1r09_i8nJ9c1nliXVGXwSqRYqklcHd9e2\n\n    **NOTE**\n    - A caution while downloading from drive. It is better to download single files since creating a zip might not include files <500 MB. This has been observed mutiple times.\n    - After downloading the files and adding them to the appropriate folder, the path of the folder can be given as input tu the data_dir path.\n\n                    Manual data can be loaded with:\n                     datasets.load_dataset(\"medical_dialog\", data_dir=\"<path/to/manual/data>\")"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"medical_dialog\", \"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c522786-aa45-4f2e-ae3e-e3646576f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a01f9-0eb8-4e99-81e3-1dcb01fcd1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
